{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook just gets tweets. 10,050 of them, English, no retweets, full text. Contain `'amazon OR Amazon OR @amazon'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Twython(app_key=TWITTER_APP_KEY, \n",
    "            app_secret=TWITTER_APP_KEY_SECRET, \n",
    "            oauth_token=TWITTER_ACCESS_TOKEN, \n",
    "            oauth_token_secret=TWITTER_ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Batch Of Data - Was Not Ultimately Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staged in 100 get requests, across enough time that it doesn't go over the free Twitter API threshold. The request updates itself each time such that the `until_id` becomes the last ID in the previous request. In other words, it gets the most recent 100 tweets, then gets the next 100 after them, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# last_id=None\n",
    "# for i in range(100):\n",
    "#     search = t.search(q='@amazon', until_id=last_id, count=100)\n",
    "#     with open(f'data/data_{i}.txt', 'w') as outfile:\n",
    "#         json.dump(search, outfile)\n",
    "#     last_id = search['statuses'][-1]['id']\n",
    "#     time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset above was ultimately not used because of a few parameter oversights on my part. Ultimately, a `WSAECONNRESET` error also caused the loop to terminate early, after 88 of 100 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Batch Of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost same process as the previous batch. Now controlling for language, no retweets, different mentions of \"Amazon\", changing the batch size, and getting \"extended\" tweets (AKA longer than 140 chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id = None\n",
    "\n",
    "query = '@amazon OR amazon OR Amazon -filter:retweets'\n",
    "\n",
    "for i in range(67):\n",
    "    search = t.search(q = query,\n",
    "                      max_id = last_id,\n",
    "                      lang = 'en',\n",
    "                      tweet_mode = 'extended',\n",
    "                      count = 150)\n",
    "    with open(f'data_no_RT/data_{i}.txt', 'w') as outfile:\n",
    "        json.dump(search, outfile)\n",
    "    last_id = search['statuses'][-1]['id']\n",
    "    time.sleep(310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Response From The Above Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = t.search(q='@amazon OR amazon OR Amazon -filter:retweets',\n",
    "                  max_id=None,\n",
    "                  lang='en',\n",
    "                  tweet_mode='extended',\n",
    "                  count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mood: \"Radio Silence (Guitar Version;2009 Remastered Version)\" by Thomas Dolby. https://t.co/3dE7Guw2Rw \n",
      "\n",
      "=========\n",
      "\n",
      "Someone take Heatherâ€™s Amazon away plz, sheâ€™s getting at least a package A DAY ðŸ¤¦ðŸ¼â€â™€ï¸ðŸ˜‚ðŸ˜‚ðŸ˜‚ \n",
      "\n",
      "=========\n",
      "\n",
      "@mitchostheleo Thatâ€™s incredibly serious and no hope that Bolsinaro will do anything to protect the Amazon. \n",
      "\n",
      "=========\n",
      "\n",
      "@Alison_McGovern @birdonthewire3 There was a time, way back, when the best graduates, wanted to work in the civil service, especially the treasury and foreign office. Now the best graduates head for Amazon, Google and Goldman Sachs. \n",
      "\n",
      "=========\n",
      "\n",
      "Apparently it's not just Amazon, this came in today via @UPS and it's the same big box for something very small. There's probably an explanation for this. https://t.co/PNODXdDckZ https://t.co/vlv4Bx1NcM \n",
      "\n",
      "=========\n",
      "\n",
      "NOW in Audio! One Sweet Christmas written by @CharlotteKent20 and narrated by @JulieBealVO https://t.co/mKn8xxEVQe and https://t.co/8pZI5P5HXf or https://t.co/7AeBfMbyJ6 #Christmas #Romance #BookBoost #TW4RW #SNRTG #RomHero #IARTG #authorRT https://t.co/6b7Rd9Ryu5 \n",
      "\n",
      "=========\n",
      "\n",
      "Hey teachers...there are so many ways to add #STEM to your classroom, no matter what subject you teach.\n",
      "It just takes a little imagination!\n",
      "\n",
      "(From the book Daily STEM âž¡ï¸https://t.co/7o842OLQR6\n",
      "#DailySTEM #Imagination https://t.co/YBcfdUDTvi \n",
      "\n",
      "=========\n",
      "\n",
      "@hypervisible And sucking in Chinese imports to sell on Amazon to fund his faltering rocket program. \n",
      "\n",
      "=========\n",
      "\n",
      "Amazon US, DM To Grab it https://t.co/c1OzMzkIDg \n",
      "\n",
      "=========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(search['statuses'][i]['full_text'], '\\n\\n=========\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Batch Of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I set the `count` on the previous query to 150, when Twitter will only allow a max of 100 per get request. To get my data up to at least 10,000 rows, I'm supplementing with 35 more get requests of 100 each, starting from the final tweet in the previous file (stored in `data_no_RT/data_66.txt`).\n",
    "\n",
    "> *Note: The reason I'm not just redoing the second batch correctly and deleting the cells that are there now is that the Twitter API limits how much you can pull per app/month/etc. I don't wanna overstay my welcome in their humble abode, so I'm adding to the previous batch with a supplementary one, instead of just redoing it all and spending more of my API bucks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_no_RT/data_66.txt') as f:\n",
    "    data_66 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id = data_66['statuses'][-1]['id']\n",
    "\n",
    "query = '@amazon OR amazon OR Amazon -filter:retweets'\n",
    "\n",
    "for i in range(35):\n",
    "    search = t.search(q = query,\n",
    "                      max_id = last_id,\n",
    "                      lang = 'en',\n",
    "                      tweet_mode = 'extended',\n",
    "                      count = 100)\n",
    "    file_num = 67 + i\n",
    "    with open(f'data_no_RT/data_{file_num}.txt', 'w') as outfile:\n",
    "        json.dump(search, outfile)\n",
    "    last_id = search['statuses'][-1]['id']\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the data is all ready and saved in the folder `data_no_RT`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
